{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory code\n",
    "\n",
    "The software in this file represents some of our explorations and experiments that were used to make our final analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Schema\n",
    "from tabulate import tabulate\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(\"../data/schema.csv\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# start spark with 1 worker thread\n",
    "sc = SparkContext(\"local[*]\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on machine events (Q1 & Q2)\n",
    "\n",
    "This file describes events that occurred to the machines in the cluster, as well as their specifications (initial events).\n",
    "\n",
    "First, let us see the schema of this file's data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(schema.get_table_schema('machine_events')['fields'],headers='keys'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Event RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events_get = schema.field_getters('machine_events')\n",
    "machine_events = schema.load_rdd(sc,'machine_events')\n",
    "machine_events.cache()\n",
    "\n",
    "for elem in machine_events.take(5):\n",
    "\tprint(elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of CPU usage\n",
    "\n",
    "For the following, we only account for the machine creation events. We note that this method could count a given machine more than once, should it happen to be added more than once (and removed in-between) or modified.\n",
    "\n",
    "Distribution of machine capacity based on their CPU power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "cpu = machine_events_get['CPUs']\n",
    "event = machine_events_get['event type']\n",
    "machine_id = machine_events_get['machine ID']\n",
    "time = machine_events_get['time']\n",
    "\n",
    "cpu_usage = machine_events \\\n",
    "    .filter(lambda row: event(row) == 0 and cpu(row) is not None) \\\n",
    "    .map(lambda row: (machine_id(row),row)) \\\n",
    "    .reduceByKey(lambda r1,r2: r1 if time(r1)>time(r2) else r2) \\\n",
    "    .map(lambda row: (cpu(row[1]), 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .sortBy(lambda x: x[0])\n",
    "\n",
    "for usage,nb in cpu_usage.collect():\n",
    "    print(f\"{nb} machines have CPU {usage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "legend = cpu_usage.map(lambda x: str(x[0]) if x[0] is not None else 'None').collect()\n",
    "count = cpu_usage.sortByKey().map(lambda x: x[1]).collect()\n",
    "ax.bar(legend,count)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost Computational Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machines_history = machine_events \\\n",
    "    .map(lambda row: (machine_id(row),row)) \\\n",
    "    .aggregateByKey(None,lambda acc, e: acc+[e] if acc is not None else [e],add) \\\n",
    "    .map(lambda machine: machine[1]) \\\n",
    "\n",
    "for e in machines_history.take(10):\n",
    "    print(tabulate(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machines_history.filter(lambda m: sorted(m)).count() - machines_history.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace period starts at 600 seconds (aka $6 \\times 10^{8}$ microseconds), to obtain the total trace time, we find the latest event time and subtracted $6 \\times 10^{8}$ to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "total_trace_time = time(machine_events.max(lambda row: time(row))) - 6*10**8\n",
    "\n",
    "print(f'The total trace time is {total_trace_time} µs ≈ {ceil(total_trace_time/(10**6 * 3600))} hours ≈ {ceil(total_trace_time/(10**6 * 3600 * 24))} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this number, computing the total computation power of a given machine is a matter of subtracting the starting time of the machine and multiplying by the CPU capacity. We assume here that the CPU capacity of a machine never changes, and we remove the machines that have some cpu values at None to prevent weird events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lost_and_total_power(events):\n",
    "    # Functions and Variables\n",
    "    status = machine_events_get['event type']\n",
    "    time = machine_events_get['time']\n",
    "    start = time(events[0])\n",
    "    cpu = machine_events_get['CPUs'](events[0])\n",
    "\n",
    "    # Lost time\n",
    "    lost_time = 0\n",
    "    for i,m in enumerate(events):\n",
    "        if i==0:\n",
    "            continue\n",
    "        if status(m)==0 and status(events[i-1])==1:\n",
    "            lost_time += time(m) - time(events[i-1])\n",
    "        \n",
    "    # Total time\n",
    "    total_time = total_trace_time if (start < 6*10**8) else (total_trace_time - start + 6*10**8)\n",
    "\n",
    "    # Resulting power\n",
    "    return (lost_time*cpu,total_time*cpu)\n",
    "\n",
    "def has_none_cpu(events):\n",
    "    cpu = machine_events_get['CPUs']\n",
    "    for m in events:\n",
    "        if cpu(m) is None:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(f'There are {machines_history.filter(has_none_cpu).count()} machines with None values in their CPUs. They will not be evaluated.')\n",
    "\n",
    "power_lost_and_total = machines_history \\\n",
    "    .filter(lambda m: not has_none_cpu(m)) \\\n",
    "    .map(lost_and_total_power) \\\n",
    "    .reduce(lambda p1,p2: (p1[0]+p2[0],p1[1]+p2[1]))\n",
    "\n",
    "print(f'During the trace time, around {(power_lost_and_total[0]/power_lost_and_total[1])*100:.4f}% of the computational power was lost.\\\n",
    "\\nThe exact percentage obtained is {(power_lost_and_total[0]/power_lost_and_total[1])*100}% !')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of jobs and tasks per scheduling class (Q3)\n",
    "\n",
    "We will evaluate the distribution over **submitted** tasks and jobs. It also implies that we do not consider the distribution _over time_ but only within the _set_ of all submitted tasks and jobs.\n",
    "\n",
    "We could also consider the evolution of the scheduling class through time, looking at the event types UPDATE_PENDING and UPDATE_RUNNING. This is a different perspective where time is a key component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our analysis with jobs. They should be fewer, as there can be many tasks per job, but one job per task.\n",
    "\n",
    "Before all, the schema for the tables we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(schema.get_table_schema('job_events')['fields'], headers='keys', maxcolwidths=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, gather the events based on the task or job they refer to. We expect such lists of events to be small enough to be handled in regular python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events = schema.load_rdd(sc,'job_events')\n",
    "job_events.persist()\n",
    "\n",
    "job_events_get = schema.field_getters('job_events')\n",
    "\n",
    "print(tabulate(job_events.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the job submission event, which is when they are assigned their (initial) scheduling class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event type 0 = SUBMIT\n",
    "scheduling_events = job_events \\\n",
    "    .filter(lambda job_event: job_events_get['event type'](job_event) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation says that it is possible for a job to be submitted more than once, when it has evicted, failed or been killed but still is runnable (up to a maximal number of trials).\n",
    "\n",
    "Do we have any such situation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_with_several_scheduling_classes = scheduling_events \\\n",
    "    .map(lambda job_event: (job_events_get['job ID'](job_event), \n",
    "                            [job_events_get['scheduling class'](job_event)])) \\\n",
    "    .reduceByKey(lambda event1, event2: event1 + event2) \\\n",
    "    .filter(lambda x: len(x[1]) > 1) \\\n",
    "    .count()\n",
    "\n",
    "print(f'{jobs_with_several_scheduling_classes} jobs have more than one scheduling class.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No.\n",
    "\n",
    "Let us then proceed with gathering the scheduling classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_scheduling_classes = scheduling_events \\\n",
    "    .map(lambda event: (job_events_get['scheduling class'](event), 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .collect()\n",
    "\n",
    "for scheduling_class in job_scheduling_classes:\n",
    "    print(scheduling_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_question3(scheduling_classes, title):\n",
    "    # TODO : make this into a proper function and put it in another file (plot.py ?)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    scheduling_class = list(map(lambda x: x[0], scheduling_classes))\n",
    "    count = list(map(lambda x: x[1], scheduling_classes))\n",
    "    ax.bar(scheduling_class,count)\n",
    "    plt.xlabel(\"Scheduling classes\")\n",
    "    plt.ylabel(\"Number of jobs\")\n",
    "    plt.xticks(scheduling_class)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "bar_plot_question3(job_scheduling_classes, \"Distribution of scheduling classes for all submitted jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating the analysis for tasks\n",
    "\n",
    "Let us now do the same analysis for all tasks, irrespective of which job they belong to.\n",
    "\n",
    "Starting with the schema of the tasks events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_get = schema.field_getters('task_events')\n",
    "\n",
    "print(tabulate(\n",
    "    schema.get_table_schema('task_events')['fields'], \n",
    "    headers='keys',\n",
    "    maxcolwidths=[None, 20, None, None, 40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the data and filter for submission events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events = schema.load_rdd(sc,'task_events').persist()\n",
    "\n",
    "task_submit_events = task_events.filter(\n",
    "    lambda task_event: task_events_get['event type'](task_event) == 0)\n",
    "\n",
    "# task_headers = map(lambda field: field['content'], task_events_schema['fields'])\n",
    "# print(tabulate(task_submit_events.take(5), headers=task_headers))\n",
    "for task_event in task_submit_events.take(5):\n",
    "    print(task_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_with_several_scheduling_classes = task_submit_events \\\n",
    "    .map(lambda event: ((task_events_get['job ID'](event), task_events_get['task index'](event)), \n",
    "                        {task_events_get['scheduling class'](event)})) \\\n",
    "    .reduceByKey(lambda event1, event2: event1 | event2) \\\n",
    "    .filter(lambda x: len(x[1]) > 1)\n",
    "\n",
    "print(f'{tasks_with_several_scheduling_classes.count()} tasks have more than one scheduling class.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we conclude that, as with jobs, no task has ever seen its scheduling class changed after being resubmitted (due to eviction, failure, kill)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_scheduling_classes = task_submit_events \\\n",
    "    .map(lambda event: (task_events_get['scheduling class'](event), 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .collect()\n",
    "\n",
    "for scheduling_class in task_scheduling_classes:\n",
    "    print(scheduling_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_question3(task_scheduling_classes, \"Distribution of scheduling classes for all submitted tasks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between scheduling class and eviction of tasks (Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduling_eviction_relations = task_events \\\n",
    "    .filter(lambda event: task_events_get['event type'](event) == 2) \\\n",
    "    .map(lambda event: (task_events_get['scheduling class'](event), 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .sortBy(lambda x: x[0])\n",
    "\n",
    "for relations in scheduling_eviction_relations.collect():\n",
    "    print(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks with lower scheduling class were evicted much more than those with higher scheduling class. Note that we chose to keep all eviction events, even when a single task is evicted several times.\n",
    "\n",
    "It matches the idea that latency-sensitive tasks, represented with higher scheduling classes, need to be interrupted (namely evicted) as little as possible, so that they can be completed soon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Job -> Same machine ? (Q5)\n",
    "\n",
    "We here take into account every task with the same job, and for each index we determine the number of machine used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_machines_per_job = task_events \\\n",
    "    .map(lambda event: (task_events_get[\"job ID\"](event),{task_events_get[\"machine ID\"](event)})) \\\n",
    "    .reduceByKey(lambda machine1,machine2: machine1 | machine2) \\\n",
    "    .map(lambda job: len(job[1])) \n",
    "\n",
    "same_and_different_distribution = nb_machines_per_job \\\n",
    "    .map(lambda count: \"1\" if count==1 else \"more than 1\") \\\n",
    "    .countByValue() \\\n",
    "    .items()\n",
    "\n",
    "for stat in same_and_different_distribution:\n",
    "    print(f\"{stat[1]} tasks have {stat[0]} machines.\")\n",
    "\n",
    "final = list(nb_machines_per_job.countByValue().items())\n",
    "final.sort(key=lambda x:x[0])\n",
    "\n",
    "nb_machine, count = list(zip(*final))\n",
    "plt.plot(nb_machine, count)\n",
    "\n",
    "#Solution for cumulative function from https://www.geeksforgeeks.org/python-program-to-find-cumulative-sum-of-a-list/\n",
    "from itertools import accumulate\n",
    "import operator\n",
    " \n",
    "def cumulative_sum(input_list):\n",
    "    # Use the accumulate() function to perform a cumulative sum of the elements in the list\n",
    "    cumulative_sum_iter = accumulate(input_list, operator.add)\n",
    "    # Convert the iterator to a list and return it\n",
    "    return list(cumulative_sum_iter)\n",
    "\n",
    "cumulative_count = cumulative_sum(count)\n",
    "\n",
    "plt.plot(nb_machine,cumulative_count)\n",
    "\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it is indeed most common to have our tasks from the same job run on 1 or 2 machines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of resource request and resource usage for tasks (Q6)\n",
    "\n",
    "This topic requires to do the analysis for three different resources: CPU cores, RAM and disk space. To start with, and become acquainted with the question at hand, we start with the CPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource requests\n",
    "\n",
    "Tasks can specify resource requirements when SUBMITTED (0) and upon UPDATE_PENDING (7) and UPDATE_RUNNING (8) events. Do they actually update their resource requests ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_requests = task_events \\\n",
    "    .filter(lambda event: task_events_get['CPU request'](event) is not None \\\n",
    "        and task_events_get['memory request'](event) is not None \\\n",
    "        and task_events_get['disk space request'](event) is not None) \\\n",
    "    .map(lambda event: ((task_events_get['job ID'](event), task_events_get['task index'](event)),\n",
    "        ({task_events_get['CPU request'](event)}, \\\n",
    "        {task_events_get['memory request'](event)}, \\\n",
    "        {task_events_get['disk space request'](event)}))) \\\n",
    "    .reduceByKey(lambda r1, r2: (r1[0] | r2[0], r1[1] | r2[1], r1[2] | r2[2]))\n",
    "\n",
    "multiple_requests = resource_requests.filter(lambda event: len(event[1][0]) > 1 or len(event[1][1]) > 1 or len(event[1][2]) > 1)\n",
    "\n",
    "print(f\"There has been {multiple_requests.count()} tasks changing their resource requests during their lifetime over a total of {resource_requests.count()}, accounting for {multiple_requests.count() / resource_requests.count() * 100}%. Here is a sample of those:\")\n",
    "\n",
    "for req in multiple_requests.take(5):\n",
    "    print(f\"- task {req[0]} requested the following resources: {req[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many tasks made more than one resource request in their lifetime."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also see when the CPU request is made. Is it only done at submit time or later ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task_events.filter(lambda event: task_events_get['CPU request'](event) is not None) \\\n",
    "    .map(lambda event: ((task_events_get['job ID'](event), task_events_get['task index'](event)),\n",
    "        (task_events_get['event type'](event), task_events_get['time'](event)))) \\\n",
    "    .reduceByKey(lambda event1, event2: event1 if event1[1] < event2[1] else event2) \\\n",
    "    .map(lambda entry: entry[1][0]) \\\n",
    "    .countByValue())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the vast majority of CPU requests are made at task submission. When it is not the case, the earliest CPU request is at scheduling. To our understanding, the SCHEDULE (1) event is caused by the system, not by the user, so we choose to only keep CPU requests defined at submission."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the tasks who make several different CPU requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_changing_events = task_events \\\n",
    "    .filter(lambda event: task_events_get['CPU request'](event) is not None) \\\n",
    "    .map(lambda event: ((task_events_get['job ID'](event), task_events_get['task index'](event)),\n",
    "        {task_events_get['event type'](event)})) \\\n",
    "    .reduceByKey(lambda cpu_request1, cpu_request2: cpu_request1 | cpu_request2) \\\n",
    "    .cache()\n",
    "\n",
    "for event in cpu_changing_events.map(lambda event: frozenset(event[1])).distinct().collect():\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events \\\n",
    "    .filter(lambda event: task_events_get['CPU request'](event) is not None and task_events_get['event type'](event) == 0) \\\n",
    "    .map(lambda event: ((task_events_get['job ID'](event), task_events_get['task index'](event)), 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_requests = task_events \\\n",
    "    .filter(lambda event: task_events_get['CPU request'](event) is not None \\\n",
    "        and task_events_get['memory request'](event) is not None \\\n",
    "        and task_events_get['disk space request'](event) is not None \\\n",
    "        and task_events_get['event type'](event) == 0) \\\n",
    "    .map(lambda event: ((task_events_get['job ID'](event), task_events_get['task index'](event)),\n",
    "        (task_events_get['CPU request'](event),task_events_get['memory request'](event),task_events_get['disk space request'](event)))) \\\n",
    "    .reduceByKey(lambda r1, r2: r1 if r1 == r2 else None) \\\n",
    "    .filter(lambda event: event[1] is not None) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for request in resource_requests.take(5):\n",
    "    print(request)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource usage table\n",
    "\n",
    "The resource usage information is stored in its own table. We will have to learn to use it, and also to match its data with the task events.\n",
    "\n",
    "It is to be noted that resource measurement periods might overshoot after a task is killed. We might want to remove these parts of the measurements, if they prove problematic.\n",
    "\n",
    "The documentation for the data set also specifies how the maximum measurement data is aggregated (for sub-containers), and gives a field in the table for this information. I believe, for the sake of this work, that we can spare ourselves the extra mileage of using this field, and just consider that the measurements are accurate enough."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, loading the related data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_usage_get = schema.field_getters('task_usage')\n",
    "task_usage = schema.load_rdd(sc,'task_usage')\n",
    "task_usage.cache()\n",
    "\n",
    "for elem in task_usage.take(5):\n",
    "\tprint(elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measurement records may be missing, not implying that a task is not running. Let us count how many such missing records there are, and how many tasks (ratio) are affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_fields = [\n",
    "        'start time',\n",
    "        'end time',\n",
    "        'CPU rate',\n",
    "        'canonical memory usage',\n",
    "        'local disk space usage'\n",
    "    ]\n",
    "\n",
    "def entry_converter(entry):\n",
    "    return tuple(task_usage_get[field](entry) for field in resource_fields)\n",
    "\n",
    "def aggregator(accumulator, resource_usage):\n",
    "    accumulator[0] += resource_usage[1]-resource_usage[0]\n",
    "    for i in range(2,len(resource_usage)):\n",
    "        accumulator[i-1] += resource_usage[0]*resource_usage[i]\n",
    "    return accumulator\n",
    "\n",
    "from operator import add\n",
    "def combiner(accumulator1, accumulator2):\n",
    "    return list(map(add, accumulator1, accumulator2))\n",
    "\n",
    "resources_per_task = task_usage \\\n",
    "    .map(lambda entry: ((task_usage_get['job ID'](entry), task_usage_get['task index'](entry)),\n",
    "        entry_converter(entry))) \\\n",
    "    .aggregateByKey([0,]*(len(resource_fields)-1), aggregator, combiner) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {resources_per_task.count()} tasks for which we have resource consumption data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in resources_per_task.take(1):\n",
    "    print(entry[0])\n",
    "    # print(entry[1])\n",
    "    for x in entry[1]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = resource_requests \\\n",
    "    .join(resources_per_task) \\\n",
    "    .map(lambda x: (x[1][0], *x[1][1])) \\\n",
    "    .sortBy(lambda x: x[0]) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in joined.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `joined`, the first element is a tuple of the requests for CPU, RAM, and storage space, and the following is respectively\n",
    "\n",
    "- the cumulated duration of the task,\n",
    "- the total amount of CPU.s for the task,\n",
    "- the RAM used,\n",
    "- the disk storage used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_req_vs_coresec = joined \\\n",
    "    .map(lambda x: (x[0][0],(x[2],1))) \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .sortByKey()\\\n",
    "    .collect()\n",
    "\n",
    "mem_req_vs_coresec = joined \\\n",
    "    .map(lambda x: (x[0][1],(x[3],1))) \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .sortByKey()\\\n",
    "    .collect()\n",
    "\n",
    "disk_req_vs_coresec = joined \\\n",
    "    .map(lambda x: (x[0][2],(x[4],1))) \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .sortByKey()\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=5\n",
    "\n",
    "plt.scatter(*zip(*cpu_req_vs_coresec),s=S)\n",
    "plt.scatter(*zip(*mem_req_vs_coresec),s=S)\n",
    "plt.scatter(*zip(*disk_req_vs_coresec),s=S)\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the code `.map(lambda x: (x[0],x[1][0]/x[1][1]))` performs averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_req_vs_core = joined \\\n",
    "    .map(lambda x: (x[0][0],(x[2]/x[1],1))) \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .sortByKey()\\\n",
    "    .collect()\n",
    "\n",
    "mem_req_vs_core = joined \\\n",
    "    .map(lambda x: (x[0][1],(x[3]/x[1],1))) \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .sortByKey()\\\n",
    "    .collect()\n",
    "\n",
    "disk_req_vs_core = joined \\\n",
    "    .map(lambda x: (x[0][2],(x[4]/x[1],1))) \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])) \\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .sortByKey()\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*zip(*cpu_req_vs_core),s=S)\n",
    "plt.scatter(*zip(*mem_req_vs_core),s=S)\n",
    "plt.scatter(*zip(*disk_req_vs_core),s=S)\n",
    "#plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspl = plt.subplot(2, 2, 1)\n",
    "rspl.scatter(*zip(*cpu_req_vs_core),s=S)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"CPU\")\n",
    "rspl = plt.subplot(2, 2, 2)\n",
    "rspl.scatter(*zip(*mem_req_vs_core),s=S)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"memory\")\n",
    "rspl = plt.subplot(2, 2, 3)\n",
    "rspl.scatter(*zip(*disk_req_vs_core),s=S)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.title(\"storage\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots shows that, in general, there are no trends regarding the CPU and Disk requests vs their real usage. However, we can clearly see that there is one for the memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between resource usage peaks and task evictions (Q7)\n",
    "\n",
    "We will investigate, for any given machine, whether there is a correlation  between resource consumption peaks and task evictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "To find a correlation, we will first construct a timeline, for each machine, of the maximum resource used, and the eviction events. Fortunately, the table Task Usage contains data on the maximum resource used withing a given time window. As such, we just need to correlate these resources with the eviction events. The documentation does not mention that tasks are migrated between machines, so we do not have to account for that.\n",
    "\n",
    "Moreover, in the documentation we learn that the traces in our task usage data are done withing periods of 300 seconds starting at 600 seconds in the timeline. As such, to have a dataset easier to look at and perform operations on, we will form our timeline in epochs starting from 0. The first epoch 0 will therefore belongs to the time frame between 600s and 900s, epoch 1 will be between 900s and 1200s and so on.\n",
    "\n",
    "Doing so, we find that there are 17 epochs in total for both task usage and task events.\n",
    "\n",
    "Below are some tests showing that no task usage recording are in between epochs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_epochs = task_usage \\\n",
    "    .map(lambda task: ((task_usage_get['start time'](task), task_usage_get['end time'](task)), 0)) \\\n",
    "    .filter(lambda x: x[0][0]//300000000!=(x[0][1]-1)//300000000).count()\n",
    "    # The -1 on the end time is to account for the events that ends at the end of an epoch (ex : 675s -> 900s is still withing epoch 0)\n",
    "    \n",
    "\n",
    "print(f\"The number of events not inside a 300s period or epoch is {overlapping_epochs}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering task consumption peaks for all machines\n",
    "\n",
    "The machine resource table contains machine IDs, so we can gather task consumption peaks for all machines on the one hand, and then task eviction events for all machines on the other hand. We can assume that no task can trigger two events at the same time, so we can ignore job ID and task index and only keep the machine ID.\n",
    "\n",
    "We gather all maximum resource values for each machine, and their epochs. This should allow to later plot, or do some form of aggregation, for instance the count of values above a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_to_peak_resources(event):\n",
    "    return {(task_usage_get['start time'](event)-600000000)//300000000 : \n",
    "                (task_usage_get['maximum CPU rate'](event), \n",
    "                task_usage_get['maximum memory usage'](event), \n",
    "                task_usage_get['maximum disk IO time'](event))\n",
    "            }\n",
    "\n",
    "def fuse_max_dictionary(d1,d2):\n",
    "    for element in d2:\n",
    "        if element in d1:\n",
    "            a1, a2, a3 = d1[element]\n",
    "            b1, b2, b3 = d2[element]\n",
    "            d1[element] = (max(a1,b1),max(a2,b2),max(a3,b3))\n",
    "        else :\n",
    "            d1[element] = d2[element]\n",
    "    return d1\n",
    "\n",
    "default_dict = {0:(.0,.0,.0),\n",
    "                1:(.0,.0,.0),\n",
    "                2:(.0,.0,.0),\n",
    "                3:(.0,.0,.0),\n",
    "                4:(.0,.0,.0),\n",
    "                5:(.0,.0,.0),\n",
    "                6:(.0,.0,.0),\n",
    "                7:(.0,.0,.0),\n",
    "                8:(.0,.0,.0),\n",
    "                9:(.0,.0,.0),\n",
    "                10:(.0,.0,.0),\n",
    "                11:(.0,.0,.0),\n",
    "                12:(.0,.0,.0),\n",
    "                13:(.0,.0,.0),\n",
    "                14:(.0,.0,.0),\n",
    "                15:(.0,.0,.0),\n",
    "                16:(.0,.0,.0)\n",
    "                }\n",
    "\n",
    "machines_max_usage = task_usage \\\n",
    "    .filter(lambda event: task_usage_get['maximum memory usage'](event) is not None and \\\n",
    "        task_usage_get['maximum CPU rate'](event) is not None and \\\n",
    "        task_usage_get['maximum disk IO time'](event) is not None \\\n",
    "    ) \\\n",
    "    .map(lambda event: \n",
    "        (task_usage_get['machine ID'](event),\n",
    "        event_to_peak_resources(event))) \\\n",
    "    .foldByKey(default_dict,lambda measurement1, measurement2: fuse_max_dictionary(measurement1,measurement2)) \\\n",
    "    .map(lambda machine: (machine[0],\n",
    "            ([k for k,_ in machine[1].items()],\n",
    "            [v[0] for _,v in machine[1].items()],\n",
    "            [v[1] for _,v in machine[1].items()],\n",
    "            [v[2] for _,v in machine[1].items()])\n",
    "            )\n",
    "        )\\\n",
    "    .cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering task eviction events for all machines\n",
    "\n",
    "So, we mapped machine IDs to the related resource information we have. Let us now map machine IDs to eviction events. Joining the two should give us something we can analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import histogram,arange\n",
    "\n",
    "task_evict_events = task_events \\\n",
    "    .filter(lambda task_event: task_events_get['event type'](task_event) == 2) \\\n",
    "    .map(lambda event: (\n",
    "            task_events_get['machine ID'](event),\n",
    "            [(\n",
    "                (task_events_get['time'](event)-600000000)//300000000\n",
    "            )]\n",
    "        )\n",
    "    ) \\\n",
    "    .reduceByKey(lambda events1, events2: events1 + events2) \\\n",
    "    .map(lambda machine : (machine[0],list(histogram(machine[1],arange(18))[0]))) \\\n",
    "    .cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Study\n",
    "\n",
    "Now, how do we associate these two sets of values ? A simple join is enough as we need to correlate datasets with the same machine ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = machines_max_usage.join(task_evict_events).map(lambda x: (x[0], (*x[1][0],x[1][1]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in correlation.sortBy(lambda x: sum(x[1][4]),ascending=False).take(5):\n",
    "    print(f\"Table for machine #{e[0]}\")\n",
    "    print(tabulate(e[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data for each machines, we need to correlate the eviction events and max resource usage for each periods. To do so we will first try to determine the correlation coefficient (also known as ***Pearson Correlation Coefficient***) for each machine and each resource usage. To do so we use the already existing *numpy* function *corrcoef*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "coef = correlation.map(lambda e: ( \\\n",
    "    [np.corrcoef(np.array(e[1][4]),np.array(e[1][1]))[1][0]],\\\n",
    "    [np.corrcoef(np.array(e[1][4]),np.array(e[1][2]))[1][0]],\\\n",
    "    [np.corrcoef(np.array(e[1][4]),np.array(e[1][3]))[1][0]] \\\n",
    ")).reduce(lambda a,b: (a[0]+b[0],a[1]+b[1],a[2]+b[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean,median\n",
    "\n",
    "max_coef = tuple([max(e) for e in coef])\n",
    "min_coef = tuple([min(e) for e in coef])\n",
    "average_coef = tuple([mean(e) for e in coef])\n",
    "median_coef = tuple([median(e) for e in coef])\n",
    "\n",
    "print(f\"Maximum coefficients found : {max_coef}\\nMinimum coefficients found : {min_coef}\\nAverage of all coefficients : {average_coef}\\nMedian coefficients : {median_coef}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are machines that seems to have correlations between the maximum resource used and the number of evictions, but the whole data (especially the mean and median) suggest that no such correlation exists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't try to look at the coefficients machine by machine, but try to find the correlation coefficient of the whole data set (while keeping X being the maximum resource used and Y the number of eviction), we obtain the following numbers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = correlation.map(lambda e: (e[1][1],e[1][4])).reduce(lambda a,b: (a[0]+b[0],a[1]+b[1]))\n",
    "memory = correlation.map(lambda e: (e[1][2],e[1][4])).reduce(lambda a,b: (a[0]+b[0],a[1]+b[1]))\n",
    "disk = correlation.map(lambda e: (e[1][3],e[1][4])).reduce(lambda a,b: (a[0]+b[0],a[1]+b[1]))\n",
    "\n",
    "print(np.corrcoef(np.array(cpu[0]),np.array(cpu[1]))[1][0])\n",
    "print(np.corrcoef(np.array(memory[0]),np.array(memory[1]))[1][0])\n",
    "print(np.corrcoef(np.array(disk[0]),np.array(disk[1]))[1][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, we can safely conclude that there are no correlation, negative of positive, between the picks of resources of a machine and its eviction events. \n",
    "\n",
    "Just for the sake of eye candy, here are the plots for each resource types maximum usage vs task evictions, where we can clearly see no correlations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*cpu)\n",
    "plt.title(\"Max CPU vs Evictions\")\n",
    "plt.xlabel(\"Maximum CPU usage of Epoch\")\n",
    "plt.ylabel(\"Number of evictions in Epoch\")\n",
    "plt.show()\n",
    "plt.scatter(*memory)\n",
    "plt.title(\"Max memory vs Evictions\")\n",
    "plt.xlabel(\"Maximum memory usage of Epoch\")\n",
    "plt.ylabel(\"Number of evictions in Epoch\")\n",
    "plt.show()\n",
    "plt.scatter(*disk)\n",
    "plt.title(\"Max disk vs Evictions\")\n",
    "plt.xlabel(\"Maximum disk usage of Epoch\")\n",
    "plt.ylabel(\"Number of evictions in Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate approach with SparkSQL, allowing the use of DataFrames:\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local(*]\") \\\n",
    "    .appName(\"Cluster analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With DataFrames\n",
    "\n",
    "It would be nice to have the tabular methods provided by DataFrames. Fortunately, Spark offers a DataFrame API, through their Spark SQL. To [load a CSV as a DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "field_to_spark_type = {\n",
    "    'time': LongType,\n",
    "    'machine ID': LongType,\n",
    "    'event type': IntegerType,\n",
    "    'platform ID': StringType,\n",
    "    'CPUs': FloatType,\n",
    "    'Memory': FloatType\n",
    "}\n",
    "machine_events_schema = StructType([\n",
    "    StructField(field['content'].replace(' ', '_'), field_to_spark_type[field['content']](), field['mandatory']) for field in schemas['machine_events']['fields']\n",
    "])\n",
    "\n",
    "machine_events = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .schema(machine_events_schema) \\\n",
    "    .load(\"../data/machine_events/part-00000-of-00001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the schema is as we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the first few data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in machine_events.take(5):\n",
    "\tprint(elem.asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much events do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.count(), machine_events.filter(machine_events.event_type == 1).count(),machine_events.filter(machine_events.event_type == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.filter(machine_events.event_type == 0).CPUs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SparkProject-DTXSDDnr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf96d2e1563c5703f4751942328699d80d1199668a3d2612f12bdd9a534862d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
