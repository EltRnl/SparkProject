{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory code\n",
    "\n",
    "The software in this file represents some of our explorations and experiments that were used to make our final analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "job_events (job_events/part-?????-of-?????.csv.gz):\n",
      "  field number  content           format       mandatory    formatter\n",
      "--------------  ----------------  -----------  -----------  ----------------------------------------------------------------\n",
      "             0  time              INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692402680>\n",
      "             1  missing info      INTEGER      False        <function parse_schema_line.<locals>.<lambda> at 0x7f86924031c0>\n",
      "             2  job ID            INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403250>\n",
      "             3  event type        INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924032e0>\n",
      "             4  user              STRING_HASH  False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692084e50>\n",
      "             5  scheduling class  INTEGER      False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403130>\n",
      "             6  job name          STRING_HASH  False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403490>\n",
      "             7  logical job name  STRING_HASH  False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403520>\n",
      "#########################################################################\n",
      "task_events (task_events/part-?????-of-?????.csv.gz):\n",
      "  field number  content                         format       mandatory    formatter\n",
      "--------------  ------------------------------  -----------  -----------  ----------------------------------------------------------------\n",
      "             0  time                            INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403370>\n",
      "             1  missing info                    INTEGER      False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403400>\n",
      "             2  job ID                          INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924035b0>\n",
      "             3  task index                      INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403640>\n",
      "             4  machine ID                      INTEGER      False        <function parse_schema_line.<locals>.<lambda> at 0x7f86924036d0>\n",
      "             5  event type                      INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403760>\n",
      "             6  user                            STRING_HASH  False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403910>\n",
      "             7  scheduling class                INTEGER      False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403880>\n",
      "             8  priority                        INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924037f0>\n",
      "             9  CPU request                     FLOAT        False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403a30>\n",
      "            10  memory request                  FLOAT        False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403ac0>\n",
      "            11  disk space request              FLOAT        False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403b50>\n",
      "            12  different machines restriction  BOOLEAN      False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403d00>\n",
      "#########################################################################\n",
      "machine_events (machine_events/part-00000-of-00001.csv.gz):\n",
      "  field number  content      format       mandatory    formatter\n",
      "--------------  -----------  -----------  -----------  ----------------------------------------------------------------\n",
      "             0  time         INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924039a0>\n",
      "             1  machine ID   INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403c70>\n",
      "             2  event type   INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403be0>\n",
      "             3  platform ID  STRING_HASH  False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403eb0>\n",
      "             4  CPUs         FLOAT        False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403d90>\n",
      "             5  Memory       FLOAT        False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438040>\n",
      "#########################################################################\n",
      "machine_attributes (machine_attributes/part-00000-of-00001.csv.gz):\n",
      "  field number  content            format                  mandatory    formatter\n",
      "--------------  -----------------  ----------------------  -----------  ----------------------------------------------------------------\n",
      "             0  time               INTEGER                 True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403e20>\n",
      "             1  machine ID         INTEGER                 True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692438160>\n",
      "             2  attribute name     STRING_HASH             True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403f40>\n",
      "             3  attribute value    STRING_HASH_OR_INTEGER  False        <function string_hash_or_integer at 0x7f86a852a9e0>\n",
      "             4  attribute deleted  BOOLEAN                 True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692438280>\n",
      "#########################################################################\n",
      "task_constraints (task_constraints/part-?????-of-?????.csv.gz):\n",
      "  field number  content              format                  mandatory    formatter\n",
      "--------------  -------------------  ----------------------  -----------  ----------------------------------------------------------------\n",
      "             0  time                 INTEGER                 True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924381f0>\n",
      "             1  job ID               INTEGER                 True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692438310>\n",
      "             2  task index           INTEGER                 True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924380d0>\n",
      "             3  comparison operator  INTEGER                 True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924383a0>\n",
      "             4  attribute name       STRING_HASH             True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692438550>\n",
      "             5  attribute value      STRING_HASH_OR_INTEGER  False        <function string_hash_or_integer at 0x7f86a852a9e0>\n",
      "#########################################################################\n",
      "task_usage (task_usage/part-?????-of-?????.csv.gz):\n",
      "  field number  content                          format    mandatory    formatter\n",
      "--------------  -------------------------------  --------  -----------  ----------------------------------------------------------------\n",
      "             0  start time                       INTEGER   True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692438430>\n",
      "             1  end time                         INTEGER   True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924384c0>\n",
      "             2  job ID                           INTEGER   True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692438670>\n",
      "             3  task index                       INTEGER   True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924385e0>\n",
      "             4  machine ID                       INTEGER   True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692438700>\n",
      "             5  CPU rate                         FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438820>\n",
      "             6  canonical memory usage           FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f86924388b0>\n",
      "             7  assigned memory usage            FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438940>\n",
      "             8  unmapped page cache              FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f86924389d0>\n",
      "             9  total page cache                 FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438a60>\n",
      "            10  maximum memory usage             FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438af0>\n",
      "            11  disk I/O time                    FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438b80>\n",
      "            12  local disk space usage           FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438c10>\n",
      "            13  maximum CPU rate                 FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438ca0>\n",
      "            14  maximum disk IO time             FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438d30>\n",
      "            15  cycles per instruction           FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438dc0>\n",
      "            16  memory accesses per instruction  FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438e50>\n",
      "            17  sample portion                   FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438ee0>\n",
      "            18  aggregation type                 BOOLEAN   False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692439090>\n",
      "            19  sampled CPU usage                FLOAT     False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692439000>\n"
     ]
    }
   ],
   "source": [
    "from schema import data_schemas_from_file\n",
    "\n",
    "schemas = data_schemas_from_file(\"../data/schema.csv\")\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "for key,value in schemas.items():\n",
    "    print(\"#########################################################################\")\n",
    "    print(f\"{key} ({value['file pattern']}):\")\n",
    "    print(tabulate(value['fields'],headers='keys'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 10:47:15 WARN Utils: Your hostname, Kixus-k resolves to a loopback address: 127.0.1.1; using 130.190.49.193 instead (on interface wlp4s0)\n",
      "22/12/01 10:47:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 10:47:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# start spark with 1 worker thread\n",
    "sc = SparkContext(\"local[*]\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate approach with SparkSQL, allowing the use of DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local(*]\") \\\n",
    "    .appName(\"Cluster analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and pre-processing machine events\n",
    "\n",
    "This file describes events that occurred to the machines in the cluster, as well as their specifications (initial events).\n",
    "\n",
    "First, let us see the schema of this file's data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  field number  content      format       mandatory    formatter\n",
      "--------------  -----------  -----------  -----------  ----------------------------------------------------------------\n",
      "             0  time         INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f86924039a0>\n",
      "             1  machine ID   INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403c70>\n",
      "             2  event type   INTEGER      True         <function parse_schema_line.<locals>.<lambda> at 0x7f8692403be0>\n",
      "             3  platform ID  STRING_HASH  False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403eb0>\n",
      "             4  CPUs         FLOAT        False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692403d90>\n",
      "             5  Memory       FLOAT        False        <function parse_schema_line.<locals>.<lambda> at 0x7f8692438040>\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(schemas['machine_events']['fields'],headers='keys'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/machine_events/part-00000-of-00001.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_events = sc.textFile(\"data/machine_events/part-00000-of-00001.csv\")\n",
    "machine_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple approach works, but it would be nice to have the tabular methods provided by DataFrames. Fortunately, Spark offers a DataFrame API, through their Spark SQL. To [load a CSV as a DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o81.csv.\n: scala.MatchError: TimestampNTZType (of class org.apache.spark.sql.types.TimestampNTZType$)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.externalDataTypeFor(RowEncoder.scala:240)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.externalDataTypeForInput(RowEncoder.scala:236)\n\tat org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType.<init>(objects.scala:1890)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.$anonfun$serializerFor$3(RowEncoder.scala:197)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.serializerFor(RowEncoder.scala:192)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:73)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:92)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:444)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m field_to_spark_type \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m: TimestampNTZType,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmachine ID\u001b[39m\u001b[39m'\u001b[39m: LongType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMemory\u001b[39m\u001b[39m'\u001b[39m: FloatType\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m machine_events_schema \u001b[39m=\u001b[39m StructType([\n\u001b[1;32m     12\u001b[0m     StructField(field[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m], field_to_spark_type[field[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]](), field[\u001b[39m'\u001b[39m\u001b[39mmandatory\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m schemas[\u001b[39m'\u001b[39m\u001b[39mmachine_events\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfields\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m ])\n\u001b[0;32m---> 14\u001b[0m machine_events \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39m../data/machine_events/part-00000-of-00001.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, schema \u001b[39m=\u001b[39;49m machine_events_schema)\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m machine_events\u001b[39m.\u001b[39mtake(\u001b[39m5\u001b[39m):\n\u001b[1;32m     16\u001b[0m \t\u001b[39mprint\u001b[39m(elem)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    536\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o81.csv.\n: scala.MatchError: TimestampNTZType (of class org.apache.spark.sql.types.TimestampNTZType$)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.externalDataTypeFor(RowEncoder.scala:240)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.externalDataTypeForInput(RowEncoder.scala:236)\n\tat org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType.<init>(objects.scala:1890)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.$anonfun$serializerFor$3(RowEncoder.scala:197)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.serializerFor(RowEncoder.scala:192)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:73)\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:92)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:444)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, TimestampNTZType, FloatType, IntegerType, LongType\n",
    "field_to_spark_type = {\n",
    "    'time': TimestampNTZType,\n",
    "    'machine ID': LongType,\n",
    "    'event type': IntegerType,\n",
    "    'platform ID': StringType,\n",
    "    'CPUs': FloatType,\n",
    "    'Memory': FloatType\n",
    "}\n",
    "machine_events_schema = StructType([\n",
    "    StructField(field['content'], field_to_spark_type[field['content']](), field['mandatory']) for field in schemas['machine_events']['fields']\n",
    "])\n",
    "machine_events = spark.read.csv(\"../data/machine_events/part-00000-of-00001.csv\", schema = machine_events_schema)\n",
    "for elem in machine_events.take(5):\n",
    "\tprint(elem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('5_SparkProject-Tuj_Du56')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ede0eb55671c30ba8a6f7c0b4ab1936479b84180dfb1e39512798d9c89e30c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
