{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory code\n",
    "\n",
    "The software in this file represents some of our explorations and experiments that were used to make our final analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schema\n",
    "from tabulate import tabulate\n",
    "from importlib import reload\n",
    "reload(schema)\n",
    "\n",
    "schemas = schema.data_schemas_from_file(\"../data/schema.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in schemas.items():\n",
    "    print(\"#########################################################################\")\n",
    "    print(f\"{key} ({value['file pattern']}):\")\n",
    "    print(tabulate(value['fields'],headers='keys'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# start spark with 1 worker thread\n",
    "sc = SparkContext(\"local[*]\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate approach with SparkSQL, allowing the use of DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local(*]\") \\\n",
    "    .appName(\"Cluster analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and pre-processing machine events\n",
    "\n",
    "This file describes events that occurred to the machines in the cluster, as well as their specifications (initial events).\n",
    "\n",
    "First, let us see the schema of this file's data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(schemas['machine_events']['fields'],headers='keys'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With DataFrames\n",
    "\n",
    "It would be nice to have the tabular methods provided by DataFrames. Fortunately, Spark offers a DataFrame API, through their Spark SQL. To [load a CSV as a DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "field_to_spark_type = {\n",
    "    'time': LongType,\n",
    "    'machine ID': LongType,\n",
    "    'event type': IntegerType,\n",
    "    'platform ID': StringType,\n",
    "    'CPUs': FloatType,\n",
    "    'Memory': FloatType\n",
    "}\n",
    "machine_events_schema = StructType([\n",
    "    StructField(field['content'].replace(' ', '_'), field_to_spark_type[field['content']](), field['mandatory']) for field in schemas['machine_events']['fields']\n",
    "])\n",
    "\n",
    "machine_events = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .schema(machine_events_schema) \\\n",
    "    .load(\"../data/machine_events/part-00000-of-00001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the schema is as we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the first few data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in machine_events.take(5):\n",
    "\tprint(elem.asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much events do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.count(), machine_events.filter(machine_events.event_type == 1).count(),machine_events.filter(machine_events.event_type == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.filter(machine_events.event_type == 0).CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going back to RDD\n",
    "\n",
    "Can't map on DataFrames, DataFrames are weird!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events_schema = schemas['machine_events']\n",
    "machine_events = sc.textFile(\"../data/machine_events/part-00000-of-00001.csv\").map(lambda row: schema.format_row(machine_events_schema, row.split(',')))\n",
    "for elem in machine_events.take(5):\n",
    "\tprint(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following, we only account for the machine creation events. We note that this method could count a given machine more than once, should it happen to be added more than once (and removed in-between) or modified.\n",
    "\n",
    "Distribution of machine capacity based on their CPU power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "cpu = schema.index_of_field(machine_events_schema, 'CPUs')\n",
    "event = schema.index_of_field(machine_events_schema, 'event type')\n",
    "\n",
    "cpu_usage = machine_events \\\n",
    "    .filter(lambda row: row[event] == 0) \\\n",
    "    .map(lambda row: (row[cpu], 1)) \\\n",
    "    .reduceByKey(add)\n",
    "\n",
    "cpu_usage.foreach(lambda cpu: print(f\"{cpu[1]} machines have CPU {cpu[0]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_axes([0,0,1,1])\n",
    "# langs = ['None', '25%', '50%', '100%']\n",
    "# students = [23,17,35,29,12]\n",
    "# ax.bar(langs,students)\n",
    "# plt.show()\n",
    "cpu_usage = cpu_usage.filter(lambda x: x[0] is not None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "legend = cpu_usage.map(lambda x: str(x[0]) if x[0] is not None else 'None').collect()\n",
    "count = cpu_usage.sortByKey().map(lambda x: x[1]).collect()\n",
    "ax.bar(legend,count)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('5_SparkProject-Tuj_Du56')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ede0eb55671c30ba8a6f7c0b4ab1936479b84180dfb1e39512798d9c89e30c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
