{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a DataFrames version of the former work\n",
    "\n",
    "## Starting PySparkSQL\n",
    "\n",
    "This interface allows the use the tabular methods provided by DataFrames. Spark offers a DataFrame API, through their Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 10:24:52 WARN Utils: Your hostname, Kixus-k resolves to a loopback address: 127.0.1.1; using 130.190.48.180 instead (on interface wlp4s0)\n",
      "23/01/12 10:24:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 10:24:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Cluster analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Schema\n",
    "from tabulate import tabulate\n",
    "from importlib import reload\n",
    "\n",
    "schema = Schema(\"../data/schema.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a CSV file\n",
    "\n",
    "To [load a CSV as a DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "# Mapping from field type as provided by the schema file, to their SparkSQL equivalent types\n",
    "field_to_spark_type = {\n",
    "    'time': LongType,\n",
    "    'machine ID': LongType,\n",
    "    'event type': IntegerType,\n",
    "    'platform ID': StringType,\n",
    "    'CPUs': FloatType,\n",
    "    'Memory': FloatType\n",
    "}\n",
    "\n",
    "# Object representing the format of the data we will be loading, so SparkSQL can do the proper pre-processing\n",
    "machine_events_schema = StructType([\n",
    "    StructField(field['content'].replace(' ', '_'), field_to_spark_type[field['content']](), field['mandatory'])\n",
    "    for field in schema.get_table_schema('machine_events')['fields']\n",
    "])\n",
    "\n",
    "machine_events = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option(\"header\",\"false\") \\\n",
    "    .schema(machine_events_schema) \\\n",
    "    .load(\"../data/machine_events/part-00000-of-00001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the schema is as we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: long (nullable = true)\n",
      " |-- machine_ID: long (nullable = true)\n",
      " |-- event_type: integer (nullable = true)\n",
      " |-- platform_ID: string (nullable = true)\n",
      " |-- CPUs: float (nullable = true)\n",
      " |-- Memory: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "machine_events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the first few data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 0, 'machine_ID': 5, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 6, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 7, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 10, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 13, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for elem in machine_events.take(5):\n",
    "\tprint(elem.asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much events do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37780, 8957, 7380)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_events.count(), machine_events.filter(machine_events.event_type == 1).count(),machine_events.filter(machine_events.event_type == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|CPUs|count|\n",
      "+----+-----+\n",
      "|0.25|  224|\n",
      "|null|   32|\n",
      "| 0.5|19773|\n",
      "| 1.0| 1414|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creation_events = machine_events.filter(machine_events.event_type == 0)\n",
    "\n",
    "cpu_counts = creation_events.groupBy(\"CPUs\").count()\n",
    "cpu_counts.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just because we can, here is a version in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|CPUs|count(1)|\n",
      "+----+--------+\n",
      "|0.25|     224|\n",
      "|null|      32|\n",
      "| 0.5|   19773|\n",
      "| 1.0|    1414|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creation_events.createOrReplaceTempView(\"creation_events\")\n",
    "\n",
    "spark.sql(\"SELECT CPUs, COUNT(*) \"\n",
    "           \"FROM creation_events \"\n",
    "           \"GROUP BY CPUs;\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of lost computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.createOrReplaceTempView(\"machine_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_cpu_machines = spark.sql(\"SELECT DISTINCT machine_ID \"\n",
    "                             \"FROM machine_events \"\n",
    "                             \"WHERE CPUs IS NULL\")\n",
    "bad_cpu_machines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+----+------+\n",
      "|time|machine_ID|event_type|         platform_ID|CPUs|Memory|\n",
      "+----+----------+----------+--------------------+----+------+\n",
      "|   0|         5|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|         6|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|         7|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        10|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        13|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        14|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        19|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        21|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        23|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        25|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        26|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        27|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        28|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        36|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        37|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        38|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        39|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        41|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        43|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        45|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "+----+----------+----------+--------------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM machine_events \"\n",
    "    \"WHERE CPUs IS NOT NULL \"\n",
    "    # \"GROUP BY machine_ID \"\n",
    "    \"ORDER BY time \"\n",
    "    ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the maximum number of events associated with any machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|n_events|\n",
      "+--------+\n",
      "|     330|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT COUNT(*) as n_events FROM machine_events \"\n",
    "    \"WHERE CPUs IS NOT NULL \"\n",
    "    \"GROUP BY machine_ID \"\n",
    "    \"ORDER BY n_events DESC \"\n",
    "    \"LIMIT 1\"\n",
    "    ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here 330 events at most for a machine. It should be easily managable by regular python code so we can proceed with the approach below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "grouped_machine_events = machine_events \\\n",
    "    .filter(machine_events['CPUs'].isNotNull()) \\\n",
    "    .orderBy(machine_events['time']) \\\n",
    "    .groupBy(machine_events['machine_ID']) \\\n",
    "    .agg(\n",
    "        F.collect_list('time').alias('time'),\n",
    "        F.collect_list('event_type').alias('event_type'),\n",
    "        F.collect_list('CPUs').alias('CPUs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|machine_ID|                time|          event_type|                CPUs|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "|         5|[835150655707, 83...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|         6|                 [0]|                 [0]|               [0.5]|\n",
      "|         7|                 [0]|                 [0]|               [0.5]|\n",
      "|        10|[1306164355566, 1...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|        13|[2447693838527, 2...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|        14|                 [0]|                 [0]|               [0.5]|\n",
      "|        19|                 [0]|                 [0]|               [0.5]|\n",
      "|        21|[1973117638055, 1...|        [2, 2, 2, 0]|[0.5, 0.5, 0.5, 0.5]|\n",
      "|        23|[481673206369, 48...|     [2, 2, 1, 0, 0]|[0.5, 0.5, 0.5, 0...|\n",
      "|        25|[669036455186, 68...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        26|[611893094118, 70...|  [1, 0, 2, 2, 2, 0]|[0.5, 0.5, 0.5, 0...|\n",
      "|        27|                 [0]|                 [0]|               [0.5]|\n",
      "|        28|[339074705486, 88...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        36|[323263688716, 32...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|        37|                 [0]|                 [0]|               [0.5]|\n",
      "|        38|                 [0]|                 [0]|               [0.5]|\n",
      "|        39|                 [0]|                 [0]|               [0.5]|\n",
      "|        41|[993255752546, 99...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        43|[372991930737, 37...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        45|[291376434099, 29...|     [1, 0, 1, 0, 0]|[0.5, 0.5, 0.5, 0...|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "grouped_machine_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Column' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# accumulateur = df.machine_ID * 0\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39m# for t in  df.time:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39m#     accumulateur += t\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mtime\n\u001b[0;32m---> 10\u001b[0m grouped_machine_events\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39maggrégée\u001b[39m\u001b[39m'\u001b[39m, col_gen(grouped_machine_events))\u001b[39m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m, in \u001b[0;36mcol_gen\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcol_gen\u001b[39m(df):\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39;49m(F\u001b[39m.\u001b[39;49marray(df\u001b[39m.\u001b[39;49mtime)))\n\u001b[1;32m      5\u001b[0m     \u001b[39m# accumulateur = df.machine_ID * 0\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39m# for t in  df.time:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39m#     accumulateur += t\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mtime\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Column' has no len()"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Column\n",
    "\n",
    "def col_gen(df):\n",
    "    print(len(F.array(df.time)))\n",
    "    # accumulateur = df.machine_ID * 0\n",
    "    # for t in  df.time:\n",
    "    #     accumulateur += t\n",
    "    return df.time\n",
    "\n",
    "grouped_machine_events.withColumn('aggrégée', col_gen(grouped_machine_events)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|machine_ID|                time|          event_type|                CPUs|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "|         5|[835150655707, 83...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|         6|                 [0]|                 [0]|               [0.5]|\n",
      "|         7|                 [0]|                 [0]|               [0.5]|\n",
      "|        10|[1306164355566, 1...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|        13|[2447693838527, 2...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|        14|                 [0]|                 [0]|               [0.5]|\n",
      "|        19|                 [0]|                 [0]|               [0.5]|\n",
      "|        21|[1973117638055, 1...|        [2, 2, 2, 0]|[0.5, 0.5, 0.5, 0.5]|\n",
      "|        23|[481673206369, 48...|     [2, 2, 1, 0, 0]|[0.5, 0.5, 0.5, 0...|\n",
      "|        25|[669036455186, 68...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        26|[611893094118, 70...|  [1, 0, 2, 2, 2, 0]|[0.5, 0.5, 0.5, 0...|\n",
      "|        27|                 [0]|                 [0]|               [0.5]|\n",
      "|        28|[339074705486, 88...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        36|[323263688716, 32...|           [1, 0, 0]|     [0.5, 0.5, 0.5]|\n",
      "|        37|                 [0]|                 [0]|               [0.5]|\n",
      "|        38|                 [0]|                 [0]|               [0.5]|\n",
      "|        39|                 [0]|                 [0]|               [0.5]|\n",
      "|        41|[993255752546, 99...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        43|[372991930737, 37...|[2, 2, 2, 2, 2, 2...|[0.5, 0.5, 0.5, 0...|\n",
      "|        45|[291376434099, 29...|     [1, 0, 1, 0, 0]|[0.5, 0.5, 0.5, 0...|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>                               (0 + 1) / 1]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "def filter_func(iterator):\n",
    "    for pdf in iterator:\n",
    "        print(type(pdf))\n",
    "        yield pdf\n",
    "\n",
    "grouped_machine_events.mapInPandas(filter_func, grouped_machine_events.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorian/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/dataframe.py:3313: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
      "  warnings.warn(\n",
      "/home/dorian/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dorian/Logiciels/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/dorian/Logiciels/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmy_function\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m     df\u001b[39m.\u001b[39mtime[\u001b[39m0\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m grouped_machine_events\u001b[39m.\u001b[39;49mto_pandas_on_spark()\u001b[39m.\u001b[39;49mapply(my_function)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/pandas/frame.py:2471\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, args, **kwds)\u001b[0m\n\u001b[1;32m   2466\u001b[0m log_advice(\n\u001b[1;32m   2467\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIf the type hints is not specified for `apply`, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2468\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mit is expensive to infer the data type internally.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2469\u001b[0m )\n\u001b[1;32m   2470\u001b[0m limit \u001b[39m=\u001b[39m get_option(\u001b[39m\"\u001b[39m\u001b[39mcompute.shortcut_limit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2471\u001b[0m pdf \u001b[39m=\u001b[39m self_applied\u001b[39m.\u001b[39;49mhead(limit \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49m_to_internal_pandas()\n\u001b[1;32m   2472\u001b[0m applied \u001b[39m=\u001b[39m pdf\u001b[39m.\u001b[39mapply(func, axis\u001b[39m=\u001b[39maxis, args\u001b[39m=\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2473\u001b[0m psser_or_psdf \u001b[39m=\u001b[39m ps\u001b[39m.\u001b[39mfrom_pandas(applied)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/pandas/frame.py:12239\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  12233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_internal_pandas\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m  12234\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  12235\u001b[0m \u001b[39m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  12236\u001b[0m \n\u001b[1;32m  12237\u001b[0m \u001b[39m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m  12238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 12239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal\u001b[39m.\u001b[39;49mto_pandas_frame\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/pandas/utils.py:588\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    585\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m    586\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_lazy_property\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, attr_name):\n\u001b[0;32m--> 588\u001b[0m         \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, attr_name, fn(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m    589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/pandas/internal.py:1056\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m sdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1056\u001b[0m pdf \u001b[39m=\u001b[39m sdf\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[1;32m   1057\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pdf) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sdf\u001b[39m.\u001b[39mschema) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1058\u001b[0m     pdf \u001b[39m=\u001b[39m pdf\u001b[39m.\u001b[39mastype(\n\u001b[1;32m   1059\u001b[0m         {field\u001b[39m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m sdf\u001b[39m.\u001b[39mschema}\n\u001b[1;32m   1060\u001b[0m     )\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m    206\u001b[0m column_counter \u001b[39m=\u001b[39m Counter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m    208\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 10:23:56 ERROR TransportChannelHandler: Connection to eduroam-049081.grenet.fr/130.190.49.81:37819 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.shuffle.io.connectionTimeout if this is wrong.\n",
      "23/01/12 10:23:56 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from eduroam-049081.grenet.fr/130.190.49.81:37819 is closed\n",
      "23/01/12 10:23:56 ERROR OneForOneBlockFetcher: Failed while starting block fetches\n",
      "java.io.IOException: Connection from eduroam-049081.grenet.fr/130.190.49.81:37819 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "def my_function(df):\n",
    "    df.time[0]+1\n",
    "\n",
    "grouped_machine_events.to_pandas_on_spark().apply(my_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Invalid  return type with grouped aggregate Pandas UDFs: StructType([StructField('cpu', DoubleType(), True)]) is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/udf.py:198\u001b[0m, in \u001b[0;36mUserDefinedFunction.returnType\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_returnType_placeholder, StructType):\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m\n\u001b[1;32m    199\u001b[0m to_arrow_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_returnType_placeholder)\n",
      "\u001b[0;31mTypeError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m event_mapping \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mremove\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mupdate\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m      3\u001b[0m \u001b[39m@pandas_udf\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcpu double\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mlost_and_total_power\u001b[39;49m(time: pd\u001b[39m.\u001b[39;49mSeries, event_type: pd\u001b[39m.\u001b[39;49mSeries, CPUs: pd\u001b[39m.\u001b[39;49mSeries) \u001b[39m-\u001b[39;49m\u001b[39m>\u001b[39;49m \u001b[39mtuple\u001b[39;49m[\u001b[39mfloat\u001b[39;49m, \u001b[39mfloat\u001b[39;49m]:\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;49;00m CPUs[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m      6\u001b[0m     min_cpu \u001b[39m=\u001b[39;49m \u001b[39mfloat\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39minf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/functions.py:450\u001b[0m, in \u001b[0;36m_create_pandas_udf\u001b[0;34m(f, returnType, evalType)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m evalType \u001b[39m==\u001b[39m PythonEvalType\u001b[39m.\u001b[39mSQL_COGROUPED_MAP_PANDAS_UDF \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(argspec\u001b[39m.\u001b[39margs) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid function: the function in cogroup.applyInPandas \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmust take either two arguments (left, right) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor three arguments (key, left, right).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     )\n\u001b[0;32m--> 450\u001b[0m \u001b[39mreturn\u001b[39;00m _create_udf(f, returnType, evalType)\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/udf.py:74\u001b[0m, in \u001b[0;36m_create_udf\u001b[0;34m(f, returnType, evalType, name, deterministic)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_udf\u001b[39m(\n\u001b[1;32m     64\u001b[0m     f: Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Any],\n\u001b[1;32m     65\u001b[0m     returnType: \u001b[39m\"\u001b[39m\u001b[39mDataTypeOrString\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUserDefinedFunctionLike\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     70\u001b[0m     \u001b[39m# Set the name of the UserDefinedFunction object to be the name of function f\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     udf_obj \u001b[39m=\u001b[39m UserDefinedFunction(\n\u001b[1;32m     72\u001b[0m         f, returnType\u001b[39m=\u001b[39mreturnType, name\u001b[39m=\u001b[39mname, evalType\u001b[39m=\u001b[39mevalType, deterministic\u001b[39m=\u001b[39mdeterministic\n\u001b[1;32m     73\u001b[0m     )\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m udf_obj\u001b[39m.\u001b[39;49m_wrapped()\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/udf.py:286\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m wrapper\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m=\u001b[39m (\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc, \u001b[39m\"\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m wrapper\u001b[39m.\u001b[39mfunc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m wrapper\u001b[39m.\u001b[39mreturnType \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturnType  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    287\u001b[0m wrapper\u001b[39m.\u001b[39mevalType \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevalType  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    288\u001b[0m wrapper\u001b[39m.\u001b[39mdeterministic \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeterministic  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[0;32m~/Logiciels/spark-3.3.0-bin-hadoop3/python/pyspark/sql/udf.py:201\u001b[0m, in \u001b[0;36mUserDefinedFunction.returnType\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         to_arrow_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_returnType_placeholder)\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInvalid  return type with grouped aggregate Pandas UDFs: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_returnType_placeholder)\n\u001b[1;32m    204\u001b[0m         )\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_returnType_placeholder\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Invalid  return type with grouped aggregate Pandas UDFs: StructType([StructField('cpu', DoubleType(), True)]) is not supported"
     ]
    }
   ],
   "source": [
    "event_mapping = {0: 'add', 1: 'remove', 2: 'update'}\n",
    "\n",
    "@pandas_udf(\"cpu double\")\n",
    "def lost_and_total_power(time: pd.Series, event_type: pd.Series, CPUs: pd.Series) -> tuple[float, float]:\n",
    "    return CPUs[-1]\n",
    "    min_cpu = float('inf')\n",
    "    max_cpu = float('-inf')\n",
    "    for i, ev in events.items():\n",
    "        # event_to_accumulator(event)\n",
    "        # update_accumulator(event)\n",
    "        if ev['CPUs'] < min_cpu:\n",
    "            min_cpu = ev['CPUs']\n",
    "        if ev['CPUs'] > max_cpu:\n",
    "            max_cpu = ev['CPUs']\n",
    "    return pd.DataFrame({'min cpu': min_cpu, 'max cpu': max_cpu})\n",
    "\n",
    "\n",
    "grouped_machine_events \\\n",
    "    .agg(\n",
    "        lost_and_total_power(\n",
    "            machine_events['time'],\n",
    "            machine_events['event_type'],\n",
    "            machine_events['CPUs'])) \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This route seams to be a dead end. The `applyInPandas` method on grouped data seems to not allow for iterative code. How can I do this aggregation then ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation[[1]](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf)[[2]](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.applyInPandas.html#pyspark.sql.GroupedData.applyInPandas) and our experiments, it seems infeasible to use the grouped data to compute the lost computational power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In general, do tasks from the same job run on the same machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In general, do tasks from the same job run on the same machine?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5_SparkProject-Tuj_Du56",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ede0eb55671c30ba8a6f7c0b4ab1936479b84180dfb1e39512798d9c89e30c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
