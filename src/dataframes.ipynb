{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a DataFrames version of the former work\n",
    "\n",
    "## Starting PySparkSQL\n",
    "\n",
    "This interface allows the use the tabular methods provided by DataFrames. Spark offers a DataFrame API, through their Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 11:22:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Cluster analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the schema file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Schema\n",
    "from tabulate import tabulate\n",
    "from importlib import reload\n",
    "\n",
    "schema = Schema(\"../data/schema.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a CSV file\n",
    "\n",
    "To [load a CSV as a DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "# Mapping from field type as provided by the schema file, to their SparkSQL equivalent types\n",
    "field_to_spark_type = {\n",
    "    'time': LongType,\n",
    "    'machine ID': LongType,\n",
    "    'event type': IntegerType,\n",
    "    'platform ID': StringType,\n",
    "    'CPUs': FloatType,\n",
    "    'Memory': FloatType\n",
    "}\n",
    "\n",
    "# Object representing the format of the data we will be loading, so SparkSQL can do the proper pre-processing\n",
    "machine_events_schema = StructType([\n",
    "    StructField(field['content'].replace(' ', '_'), field_to_spark_type[field['content']](), field['mandatory'])\n",
    "    for field in schema.get_table_schema('machine_events')['fields']\n",
    "])\n",
    "\n",
    "machine_events = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option(\"header\",\"false\") \\\n",
    "    .schema(machine_events_schema) \\\n",
    "    .load(\"../data/machine_events/part-00000-of-00001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the schema is as we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: long (nullable = true)\n",
      " |-- machine_ID: long (nullable = true)\n",
      " |-- event_type: integer (nullable = true)\n",
      " |-- platform_ID: string (nullable = true)\n",
      " |-- CPUs: float (nullable = true)\n",
      " |-- Memory: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "machine_events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inspect the first few data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 0, 'machine_ID': 5, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 6, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 7, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 10, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n",
      "{'time': 0, 'machine_ID': 13, 'event_type': 0, 'platform_ID': 'HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=', 'CPUs': 0.5, 'Memory': 0.2493000030517578}\n"
     ]
    }
   ],
   "source": [
    "for elem in machine_events.take(5):\n",
    "\tprint(elem.asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much events do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37780, 8957, 7380)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_events.count(), machine_events.filter(machine_events.event_type == 1).count(),machine_events.filter(machine_events.event_type == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|CPUs|count|\n",
      "+----+-----+\n",
      "|0.25|  224|\n",
      "|null|   32|\n",
      "| 0.5|19773|\n",
      "| 1.0| 1414|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creation_events = machine_events.filter(machine_events.event_type == 0)\n",
    "\n",
    "cpu_counts = creation_events.groupBy(\"CPUs\").count()\n",
    "cpu_counts.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just because we can, here is a version in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|CPUs|count(1)|\n",
      "+----+--------+\n",
      "|0.25|     224|\n",
      "|null|      32|\n",
      "| 0.5|   19773|\n",
      "| 1.0|    1414|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creation_events.createOrReplaceTempView(\"creation_events\")\n",
    "\n",
    "spark.sql(\"SELECT CPUs, COUNT(*) \"\n",
    "           \"FROM creation_events \"\n",
    "           \"GROUP BY CPUs;\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of lost computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events.createOrReplaceTempView(\"machine_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_cpu_machines = spark.sql(\"SELECT DISTINCT machine_ID \"\n",
    "                             \"FROM machine_events \"\n",
    "                             \"WHERE CPUs IS NULL\")\n",
    "bad_cpu_machines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+----+------+\n",
      "|time|machine_ID|event_type|         platform_ID|CPUs|Memory|\n",
      "+----+----------+----------+--------------------+----+------+\n",
      "|   0|         5|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|         6|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|         7|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        10|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        13|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        14|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        19|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        21|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        23|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        25|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        26|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        27|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        28|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        36|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        37|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        38|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        39|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        41|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        43|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "|   0|        45|         0|HofLGzk1Or/8Ildj2...| 0.5|0.2493|\n",
      "+----+----------+----------+--------------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM machine_events \"\n",
    "    \"WHERE CPUs IS NOT NULL \"\n",
    "    # \"GROUP BY machine_ID \"\n",
    "    \"ORDER BY time \"\n",
    "    ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the maximum number of events associated with any machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|n_events|\n",
      "+--------+\n",
      "|     330|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT COUNT(*) as n_events FROM machine_events \"\n",
    "    \"WHERE CPUs IS NOT NULL \"\n",
    "    \"GROUP BY machine_ID \"\n",
    "    \"ORDER BY n_events DESC \"\n",
    "    \"LIMIT 1\"\n",
    "    ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here 330 events at most for a machine. It should be easily managable by regular python code so we can proceed with the approach below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "grouped_machine_events = machine_events \\\n",
    "    .filter(machine_events['CPUs'].isNotNull()) \\\n",
    "    .orderBy(machine_events['time']) \\\n",
    "    .groupBy(machine_events['machine_ID']) \\\n",
    "    .agg(\n",
    "        F.collect_list('time').alias('time'),\n",
    "        F.collect_list('event_type').alias('event_type'),\n",
    "        F.collect_list('CPUs').alias('CPUs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_machine_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column\n",
    "\n",
    "def col_gen(df):\n",
    "    print(len(F.array(df.time)))\n",
    "    # accumulateur = df.machine_ID * 0\n",
    "    # for t in  df.time:\n",
    "    #     accumulateur += t\n",
    "    return df.time\n",
    "\n",
    "grouped_machine_events.withColumn('aggrégée', col_gen(grouped_machine_events)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "def filter_func(iterator):\n",
    "    for pdf in iterator:\n",
    "        print(type(pdf))\n",
    "        yield pdf\n",
    "\n",
    "grouped_machine_events.mapInPandas(filter_func, grouped_machine_events.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(df):\n",
    "    df.time[0]+1\n",
    "\n",
    "grouped_machine_events.to_pandas_on_spark().apply(my_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_mapping = {0: 'add', 1: 'remove', 2: 'update'}\n",
    "\n",
    "@pandas_udf(\"cpu double\")\n",
    "def lost_and_total_power(time: pd.Series, event_type: pd.Series, CPUs: pd.Series) -> tuple[float, float]:\n",
    "    return CPUs[-1]\n",
    "    min_cpu = float('inf')\n",
    "    max_cpu = float('-inf')\n",
    "    for i, ev in events.items():\n",
    "        # event_to_accumulator(event)\n",
    "        # update_accumulator(event)\n",
    "        if ev['CPUs'] < min_cpu:\n",
    "            min_cpu = ev['CPUs']\n",
    "        if ev['CPUs'] > max_cpu:\n",
    "            max_cpu = ev['CPUs']\n",
    "    return pd.DataFrame({'min cpu': min_cpu, 'max cpu': max_cpu})\n",
    "\n",
    "\n",
    "grouped_machine_events \\\n",
    "    .agg(\n",
    "        lost_and_total_power(\n",
    "            machine_events['time'],\n",
    "            machine_events['event_type'],\n",
    "            machine_events['CPUs'])) \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This route seams to be a dead end. The `applyInPandas` method on grouped data seems to not allow for iterative code. How can I do this aggregation then ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation[[1]](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf)[[2]](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.applyInPandas.html#pyspark.sql.GroupedData.applyInPandas) and our experiments, it seems infeasible to use the grouped data to compute the lost computational power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In general, do tasks from the same job run on the same machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from field type as provided by the schema file, to their SparkSQL equivalent types\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "field_to_spark_type = {\n",
    "    'time': LongType,\n",
    "    'missing info': LongType,\n",
    "    'job ID': LongType,\n",
    "    'task index': IntegerType,\n",
    "    'machine ID': LongType,\n",
    "    'event type': IntegerType,\n",
    "    'user': StringType,\n",
    "    'scheduling class': IntegerType,\n",
    "    'priority': IntegerType,\n",
    "    'CPU request': FloatType,\n",
    "    'memory request': FloatType,\n",
    "    'disk space request': FloatType,\n",
    "    'different machines restriction': BooleanType\n",
    "}\n",
    "\n",
    "# Object representing the format of the data we will be loading, so SparkSQL can do the proper pre-processing\n",
    "task_events_schema = StructType([\n",
    "    StructField(field['content'].replace(' ', '_'), field_to_spark_type[field['content']](), field['mandatory'])\n",
    "    for field in schema.get_table_schema('task_events')['fields']\n",
    "])\n",
    "\n",
    "task_events = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option(\"header\",\"false\") \\\n",
    "    .schema(task_events_schema) \\\n",
    "    .load(\"../data/task_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------+----------+----------+----------+--------------------+----------------+--------+-----------+--------------+------------------+------------------------------+\n",
      "|time|missing_info| job_ID|task_index|machine_ID|event_type|                user|scheduling_class|priority|CPU_request|memory_request|disk_space_request|different_machines_restriction|\n",
      "+----+------------+-------+----------+----------+----------+--------------------+----------------+--------+-----------+--------------+------------------+------------------------------+\n",
      "|   0|           2|3418309|         0|4155527081|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418309|         1| 329150663|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|        null|3418314|         0|3938719206|         0|70s3v5qRyCO/1PCdI...|               3|       9|      0.125|       0.07446|          4.244E-4|                          null|\n",
      "|   0|        null|3418314|         1| 351618647|         0|70s3v5qRyCO/1PCdI...|               3|       9|      0.125|       0.07446|          4.244E-4|                          null|\n",
      "|   0|           2|3418319|         0| 431052910|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418319|         1| 257348783|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418324|         0|5655258253|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418324|         1|3550322224|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418329|         0|   1303745|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418329|         1|3894543095|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418329|         2| 336025676|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418334|         0|3405236527|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418334|         1| 431081448|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418339|         0|  84899647|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418339|         1|   1268205|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418339|         2|   7753127|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418339|         3| 351621284|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418339|         4| 317488701|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418339|         5|2595183881|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "|   0|           2|3418339|         6| 621588868|         0|70s3v5qRyCO/1PCdI...|               3|       9|       null|          null|              null|                          null|\n",
      "+----+------------+-------+----------+----------+----------+--------------------+----------------+--------+-----------+--------------+------------------+------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+\n",
      "|count(machine_ID)|count(job_ID)|\n",
      "+-----------------+-------------+\n",
      "|                0|           16|\n",
      "|                1|         3065|\n",
      "|                2|          412|\n",
      "|                3|          238|\n",
      "|                4|           74|\n",
      "|                5|          139|\n",
      "|                6|           32|\n",
      "|                7|           14|\n",
      "|                8|           47|\n",
      "|                9|           15|\n",
      "|               10|          149|\n",
      "|               11|           21|\n",
      "|               12|           23|\n",
      "|               13|            5|\n",
      "|               14|           13|\n",
      "|               15|           20|\n",
      "|               16|           15|\n",
      "|               17|            6|\n",
      "|               18|           13|\n",
      "|               20|           41|\n",
      "+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_events.groupBy(\"job_ID\") \\\n",
    "    .agg(F.count_distinct(task_events.machine_ID)) \\\n",
    "    .groupBy(\"count(machine_ID)\") \\\n",
    "    .agg({\"job_ID\":\"count\"}) \\\n",
    "    .orderBy(\"count(machine_ID)\") \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
